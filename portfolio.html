<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Projects Portfolio</title>
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet"/>
  <style>
    body {
      padding: 20px;
      transition: background-color 0.3s, color 0.3s;
    }
    .tech-list { list-style-type: disc; margin-left: 20px; }
    .sub-list { list-style-type: circle; margin-left: 40px; }
    .detail-section { margin-bottom: 1em; }
    body.dark-mode {
      background-color: #1e1e1e;
      color: #d4d4d4;
    }
    body.dark-mode a {
      color: #4aa3ff;
    }
    body.dark-mode .card {
      background-color: #2c2c2c;
      border-color: #444;
    }
    .theme-toggle {
      margin-bottom: 20px;
    }
  </style>
</head>
<body>
  <div class="container py-4">
    <!-- <button class="btn btn-secondary theme-toggle" onclick="document.body.classList.toggle('dark-mode')">Toggle Theme</button>-->
    <h1 class="mb-4">Projects Portfolio</h1>

    <!-- 1. Time Series Analysis with ARIMA/SARIMA -->
    <div class="card mb-4">
      <div class="card-body">
        <h3 class="card-title">Time Series Analysis with ARIMA and SARIMA<br><br></h3>
        <div class="detail-section">
              <p>In this study I explored and discovered models that can accurately forecast river flow using historical streamflow data obtained from USGS web interface and NOAA API requests. The forecast data obtained is strictly meant for recreational purposes but could be used for other purposes. A modular Python package automates the process of preprocessing, modeling, diagnosing, and evaluating ARIMA/SARIMA time series methodologies, and can be used for time series analysis on any fitting dataset, not just streamflow.</p>
              <p>Read the full report here: <a href="https://github.com/at58474/Time-Series-ARIMA-SARIMA/blob/main/TS_Analysis_RFG.pdf" target="_blank">Time Series Model Performance Analysis (PDF)</a></p>
            </div>
        <div>
          <p><strong>Technologies &amp; Tools</strong></p>
          <ul class="tech-list">
            <li>Python (Pandas, statsmodels, SciPy, Matplotlib, Seaborn)</li>
            <li>Facebook Prophet</li>
            <li>Auto-ARIMA &amp; Auto-SARIMA implementations</li>
            <li>NOAA API &amp; USGS National Water Dashboard API</li>
            <li>Multiprocessing</li>
            <li>SARIMAX (predict &amp; forecast methods)</li>
            <li>Custom Python modules for automated time series modeling</li>
            <li>PDF report generation for model results</li>
          </ul>

  <p><strong>Statistical Methods</strong></p>
  <ul class="tech-list">
    <li>Augmented Dickey-Fuller (ADF) test for stationarity (with auto-differencing)</li>
    <li>Autocorrelation (ACF) &amp; Partial Autocorrelation (PACF) analysis</li>
    <li>Canova-Hansen (CH) test for seasonality</li>
    <li>Ljung-Box test for residual correlation</li>
    <li>Seasonal decomposition of time series</li>
    <li>Box-Jenkins methodology for parameter estimation</li>
    <li>Residual diagnostics: standardized residuals, histograms, Q-Q plots, ACF plots</li>
  </ul>

  <p><strong>Machine Learning / Time Series Forecasting Models</strong></p>
  <ul class="tech-list">
    <li>ARIMA (Autoregressive Integrated Moving Average)</li>
    <li>SARIMA (Seasonal ARIMA)</li>
    <li>VAR (Vector Autoregression)</li>
    <li>Seasonal differencing parameter optimization</li>
  </ul>

  <p><strong>Data Processing &amp; Modeling Techniques</strong></p>
  <ul class="tech-list">
    <li>Merging multiple datasets from different sources</li>
    <li>Data cleaning: duplicate removal, timestamp reindexing, missing data imputation (forward fill), column renaming/deletion</li>
    <li>Flexible raw data ingestion with configurable delimiter</li>
    <li>Downsampling &amp; aggregation (mean, first, max) across multiple time intervals</li>
    <li>Anomaly detection &amp; handling (including extreme value retention for hydrological relevance)</li>
    <li>Parameter tuning &amp; model evaluation using AIC, RMSE, and MAPE</li>
    <li>Comparative analysis of forecasting methods and aggregation strategies</li>
    <li>In-sample non-rolling, in-sample 1-step-ahead rolling, and out-of-sample rolling forecasts</li>
    <li>Cross-validation (customizable number of groups &amp; test sizes) with visualized performance results</li>
    <li>Auto-ARIMA iterative parameter search with top-N results ranked by AIC, MAPE, and RMSE</li>
    <li>Automated plotting &amp; result storage with structured directory output</li>
    <li>File handling automation with configurable cleanup and archiving structure</li>
  </ul>
</div>

        <div class="accordion" id="accordionTS">
          <div class="accordion-item">
            <h2 class="accordion-header" id="headingTS">
              <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseTS">Full Project Details</button>
            </h2>
            <div id="collapseTS" class="accordion-collapse collapse" aria-labelledby="headingTS" data-bs-parent="#accordionTS">
              <div class="accordion-body">
                <div class="detail-section">
                  <h5>Project Overview</h5>
                  <p>For this project I developed a forecasting application for predicting river flow at the Russell Fork Gorge using historical 15-minute USGS streamflow data and NOAA precipitation records. Twenty-five datasets were generated through varying aggregation methods and time intervals. These datasets were modeled using ARIMA, SARIMA, and VAR, with parameters optimized via ADF, ACF, PACF, AIC, RMSE, and MAPE and performance validated via rolling and cross-validation techniques. Results showed that ARIMA excelled at short-term forecasts (1–7 days), while SARIMA performed better for longer seasonal patterns, with mean aggregation producing the most accurate results.</p>
                  <p>The study demonstrated that the ARIMA model, tuned through the Box–Jenkins methodology, could reliably predict streamflow hours to days in advance, supporting recreational planning and potentially informing flood control or emergency management with further refinement. Automated workflows were implemented for data cleaning, anomaly detection, parameter tuning, and results reporting, enabling efficient, reproducible, and scalable forecasting. Limitations included inconsistent precipitation data for VAR modeling, highlighting the need for improved rainfall measurements in remote areas.</p>
                </div>
                <div class="detail-section">
                  <h5>Key Features & Workflow</h5>
                  <ul class="tech-list">
                    <li><strong>Preprocessing:</strong> merging multiple datasets, imputing missing data, removing duplicates, renaming/deleting columns with configuration support.</li>
                    <li><strong>Resampling:</strong> multi-frequency resampling (first, mean, max) with optional row cap to speed processing.</li>
                    <li><strong>Stationarity & Decomposition:</strong> Automated ADF test with differencing, ACF and PACF plots, and seasonal-trend-residual decomposition visualizations.</li>
                    <li><strong>Modeling:</strong>
                      <ul>
                        <li>In-sample, rolling 1-step ahead, and out-of-sample forecasting using ARIMA and SARIMA.</li>
                        <li>Performance metrics generated for each model variant: AIC, RMSE, MAPE, comparing predict() vs forecast().</li>
                      </ul>
                    </li>
                    <li><strong>Diagnostics:</strong> Residual analysis with Q-Q plots, histograms, and autocorrelation plots using both statsmodels built-in diagnostics and custom plotting methods.</li>
                    <li><strong>Cross Validation:</strong> User-configurable cross-validation folds with visualizations of in-sample and out-of-sample forecast accuracy and error metrics across datasets.</li>
                    <li><strong>Auto-ARIMA Grid Search:</strong> Exhaustive parameter search with PDF reports summarizing top models by AIC, MAPE, and RMSE.</li>
                    <li><strong>File Management:</strong> Optional auto-cleaning of previous output files and infrastructure for archiving results.</li>
                  </ul>
                </div>
                <div class="detail-section">
                  <h5>Configuration & Control</h5>
                  <ul class="tech-list">
                    <li><code>config.py</code> – user parameters for dataset paths, model ranges, etc.</li>
                    <li><code>config_validation.py</code> – validation of config parameters with assertions.</li>
                    <li><code>controller.py</code> – orchestrates execution flow using validated settings.</li>
                  </ul>
                </div>
                <div class="detail-section">
                  <h5>Skills Gained</h5>
                  <ul class="tech-list">
                    <li>Time series theory and ARIMA/SARIMA modeling techniques</li>
                    <li>Data preprocessing, cleaning, and resampling strategies</li>
                    <li>Statistical stationarity testing and diagnostics</li>
                    <li>Automated model selection and hyperparameter tuning</li>
                    <li>Cross-validation methodologies for time series</li>
                    <li>Report generation and pipeline automation</li>
                    <li>Working with complex configuration files</li>
                    <li>Code modularization and documentation best practices</li>
                  </ul>
                </div>
                <div class="detail-section">
                  <h5>Useful Links</h5>
                  <ul class="tech-list">
                    <li><a href="https://github.com/at58474/Time-Series-ARIMA-SARIMA/blob/main/README.md" target="_blank">GitHub README</a></li>
                    <li><a href="https://github.com/at58474/Time-Series-ARIMA-SARIMA/blob/main/tsmodule.py" target="_blank">tsmodule.py (Core Python module)</a></li>
                    <li><a href="https://github.com/at58474/Time-Series-ARIMA-SARIMA/blob/main/config.py" target="_blank">config.py</a></li>
                    <li><a href="https://github.com/at58474/Time-Series-ARIMA-SARIMA/blob/main/config_validation.py" target="_blank">config_validation.py</a></li>
                    <li><a href="https://github.com/at58474/Time-Series-ARIMA-SARIMA/blob/main/controller.py" target="_blank">controller.py</a></li>
                  </ul>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- 2. Protein Data Bank Crystallization Conditions Analysis -->
    <!-- Protein Data Bank Crystallization Conditions Analysis -->
<div class="card mb-4">
  <div class="card-body">
    <h3 class="card-title">Protein Data Bank Crystallization Conditions Analysis<br><br></h3>
    <div class="detail-section">
      <p>This project analyzes crystallization conditions from the Protein Data Bank (PDB) to uncover trends, correlations, and optimal experimental parameters for protein structure determination. The workflow includes automated data extraction, cleaning, and transformation, followed by exploratory data analysis (EDA) and statistical modeling to identify influential chemical and physical factors. Visualizations and summaries highlight patterns in pH, temperature, precipitant composition, and other crystallization variables across thousands of entries.</p>
      <p><a href="https://at58474.github.io/another-page" target="_blank">Exploratory Data Analysis of Protein Crystallization Conditions</a></p>
    </div>
    <div>
      <p><strong>Technologies &amp; Tools</strong></p>
      <ul class="tech-list">
        <li>Python (Pandas, NumPy, Matplotlib, Seaborn)</li>
        <li>SciPy (statistical analysis &amp; hypothesis testing)</li>
        <li>BioPython</li>
        <li>Scikit-Learn (CNNs and classifiers)</li>
        <li>Multiprocessing</li>
        <li>Jupyter Notebooks</li>
        <li>BeautifulSoup &amp; requests (web scraping PDB data)</li>
        <li>Regex / Text processing</li>
        <li>FTP Data Download Automation</li>
        <li>SQLite database for structured storage and retrieval</li>
        <li>Custom Python scripts for data parsing and transformation</li>
        <li>Automated visualization generation and export</li>
        <li>Data Visualization &amp; Analysis</li>
      </ul>

      <p><strong>Statistical Methods</strong></p>
      <ul class="tech-list">
        <li>Correlation analysis between crystallization variables and experimental outcomes</li>
        <li>ANOVA &amp; t-tests to assess differences between condition groups</li>
        <li>Chi-square tests for categorical variables (e.g., precipitant types)</li>
        <li>Principal Component Analysis (PCA) for dimensionality reduction</li>
        <li>Clustering (k-means, hierarchical) for grouping similar crystallization profiles</li>
        <li>Outlier detection and treatment in crystallization datasets</li>
      </ul>

      <p><strong>Data Processing &amp; Analysis Techniques</strong></p>
      <ul class="tech-list">
        <li>Automated retrieval and parsing of PDB crystallization metadata</li>
        <li>Standardization of units and categorical variables (e.g., buffer types, precipitant categories)</li>
        <li>Data cleaning: missing value imputation, duplicate removal, and normalization</li>
        <li>Feature engineering (e.g., grouping related chemicals, calculating ionic strengths)</li>
        <li>Aggregation and filtering for specific protein families or resolution ranges</li>
        <li>Visualization of variable distributions, correlations, and clustering results</li>
        <li>Export of cleaned datasets, figures, and statistical summaries for publication or reuse</li>
      </ul>
    </div>


        <div class="accordion" id="accordionPDB">
          <div class="accordion-item">
            <h2 class="accordion-header" id="headingPDB">
              <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapsePDB">Full Project Details</button>
            </h2>
            <div id="collapsePDB" class="accordion-collapse collapse" aria-labelledby="headingPDB" data-bs-parent="#accordionPDB">
              <div class="accordion-body">
  <div class="detail-section">
    <h5>Project Summary</h5>
    <p>This project focuses on the large-scale extraction, cleaning, and analysis of protein crystallization condition data from the Protein Data Bank (PDB), a global repository of experimentally determined macromolecular structures. Using a dataset of 204,170 PDB files (290 GB) downloaded from the official PDB FTP servers, the project targets protein-only entries—excluding DNA, RNA, and computational models such as AlphaFold predictions—to retain only experimentally validated structures.</p>
    <p>The workflow involves automated data extraction, cleaning, and transformation, followed by exploratory data analysis (EDA) and statistical modeling to uncover key crystallization parameters, trends, correlations, and optimal experimental conditions. Visualizations and summaries highlight patterns in pH, temperature, precipitant composition, and other critical crystallization variables across thousands of entries, aiming to inform and improve experimental design for novel protein structures.</p>
  </div>
  <div class="detail-section">
    <h5>Primary Goals</h5>

      <li><strong>Data Extraction</strong> – Parse each PDB file to extract:
        <ul class="tech-list">
          <li>Protein ID</li>
          <li>FASTA sequence</li>
          <li>Solvent concentration</li>
          <li>Matthew’s coefficient</li>
          <li>Crystallography type and vapor diffusion method</li>
          <li>pH</li>
          <li>Organic and salt precipitates (categorized by concentration units)</li>
        </ul>
      </li>
      <li><strong>Data Analysis &amp; Modeling</strong> – Generate visual and statistical insights into crystallization trends, and explore deep learning-based predictive modeling for crystallization conditions from FASTA sequences.</li>

  </div>
  <div class="detail-section">
    <h5>Data Collection &amp; Preprocessing</h5>
    <ul class="tech-list">
      <li>Downloaded 204,170 .ent.gz PDB files from ftp.wwpdb.org/pub/pdb/data/structures/all</li>
      <li>Decompressed files using 7zip CLI</li>
      <li>Converted .ent to .pdb format via a multiprocessing-enabled Python script</li>
    </ul>
  </div>
  <div class="detail-section">
    <h5>Core Extraction Module: ccmodule.py (892 lines)</h5>

      <li><strong>PDBPreprocessingSequences</strong>
        <ul class="tech-list">
          <li>Extracts primary sequences from SEQRES records and converts them to FASTA format using BioPython.</li>
          <li>Stores sequences for integration into the master DataFrame via protein ID matching.</li>
          <li>Handles multiprocessing across all CPU cores for speed.</li>
          <li><em>Note:</em> SEQRES sequences may differ from ATOM-based sequences due to cleaved tags or loops, potentially impacting predictive modeling.</li>
        </ul>
      </li>
      <li><strong>PDBCrystallizationConditions</strong> – Handles condition-specific data extraction, including:
        <ul class="tech-list">
          <li>extract_remark_280() – Retrieves all crystallization notes from REMARK 280 sections.</li>
          <li>extract_solvent_content() – Solvent content (%).</li>
          <li>extract_matthews_coefficient() – Crystal packing density measure.</li>
          <li>extract_crystallography_type() – Categorizes crystallization method (e.g., Vapor Diffusion, Dialysis, Batch).</li>
          <li>extract_vapor_diffusion_type() – Distinguishes between Hanging Drop and Sitting Drop methods.</li>
          <li>extract_ph() – pH values of mother liquor solutions.</li>
          <li>extract_organic_precipitates() – Uses REGEX and fuzzy matching to capture percentage-based organic compounds, correcting typos and mislabeled entries.</li>
          <li>extract_salt_precipitates() – Identifies molarity-based inorganic salts.</li>
        </ul>
      </li>
      <li><strong>ConvertToPDB</strong>
        <ul class="tech-list">
          <li>Multiprocessing-based .ent → .pdb converter.</li>
        </ul>
      </li>

  </div>
  <div class="detail-section">
    <h5>Analytical Insights Generated</h5>
    <ul class="tech-list">
      <li>Missing Data Analysis: Tables and bar charts summarizing null distributions.</li>
      <li>Nullity Correlation Heatmap: Identifies dependencies between missing fields.</li>
      <li>Chemical Usage Statistics:
        <ul>
          <li>Comprehensive list of all unique chemicals with occurrence counts.</li>
          <li>Top 10 most common chemicals (tabular, pie chart).</li>
          <li>Scatter plots showing concentration distribution per chemical.</li>
        </ul>
      </li>
      <li>pH Trends:
        <ul>
          <li>Distribution of all recorded pH values.</li>
          <li>Grouped pH frequency charts (integer bins).</li>
        </ul>
      </li>
    </ul>
  </div>
  <div class="detail-section">
    <h5>Deep Learning Component</h5>
    <ul class="tech-list">
      <li>Objective: Predict optimal crystallization conditions (pH, temperature, chemical composition) from FASTA sequences using supervised models (e.g., CNNs).</li>
      <li>Approach: Train on extracted experimental conditions and sequence data to provide predictions for uncharacterized proteins.</li>
      <li>Potential Impact: Could accelerate experimental design and improve crystal yield for novel proteins.</li>
    </ul>
  </div>
  <div class="detail-section">
    <h5>Skills Gained</h5>
    <ul class="tech-list">
        <li>Handling massive biological datasets</li>
        <li>Advanced text parsing and regular expressions</li>
        <li>Multiprocessing and parallelization</li>
        <li>Biopython for structural bioinformatics</li>
        <li>Deep learning for biological sequence data</li>
        <li>Data cleaning and normalization techniques</li>
        <li>Scientific visualization</li>
        <li>Model development and evaluation</li>
    </ul>
  </div>
  <div class="detail-section">
    <h5>Impact &amp; Future Directions</h5>
    <p>This work produced one of the most comprehensive datasets of protein crystallization conditions available directly from the PDB. Its structured format enables:</p>
    <ul class="tech-list">
      <li>Statistical analyses of crystallization trends.</li>
      <li>Integration into predictive modeling pipelines.</li>
    </ul>
    <p>Potential future modules include:</p>
    <ul class="tech-list">
      <li>ATOM-coordinate-based sequence extraction.</li>
      <li>Identification of structural modifications affecting crystallization.</li>
      <li>Expanded machine learning model training.</li>
    </ul>
  </div>
                  <div class="detail-section">
                  <h5>Useful Links</h5>
                  <ul class="sub-list">
                    <li><a href="https://github.com/at58474/Crystallization-Conditions" target="_blank">GitHub Repo</a></li>
                  </ul>
                </div>
</div>

            </div>
          </div>
        </div>
      </div>
    </div>


  <!-- 3. Climbing Conditions Web App -->
<div class="card mb-4">
  <div class="card-body">
    <h3 class="card-title">Climbing Conditions Web App<br><br></h3>
    <div class="detail-section">
        <p><b>Machine Learning Powered Flask App for Predicting Climbing Conditions</b> — A full-stack application that integrates live, historical, and forecasted weather data from multiple APIs, processes it with machine learning models trained on optimal climbing conditions, and generates a Climbing Conditions Score (CCS) for each location. Interactive, mobile-friendly visualizations present real-time and forecasted scores alongside key weather metrics, enabling climbers to assess conditions and plan trips with confidence</p>
      <p><a href="https://www.climbingconditions.com/" target="_blank">Live Climbing Conditions Website</a></p>
    </div>
    <div class="detail-section">
      <p><strong>Technology Stack</strong></p>

        <strong>Backend:</strong>
          <ul class="tech-list">
            <li>Python 3.x</li>
            <li>Flask</li>
            <li>SQLite3 (database engine)</li>
            <li>scikit-learn (RandomForestRegressor, DecisionTreeRegressor)</li>
            <li>joblib (model serialization/loading)</li>
            <li>requests (OpenWeatherMap API consumption)</li>
            <li>dateutil (datetime parsing and manipulation)</li>
            <li>Python logging (error tracking and debugging)</li>
            <li>Virtual environment and dependency management (e.g., venv, pip)</li>
          </ul>

        <strong>Frontend:</strong>
          <ul class="tech-list">
            <li>JavaScript (ES6, Fetch API)</li>
            <li>Plotly.js (interactive graphs)</li>
            <li>HTML5 / CSS3</li>
            <li>Bootstrap 5</li>
            <li>LocalStorage API (storing user preferences like selected destination)</li>
            <li>Event listeners and DOM manipulation (UI interactivity)</li>
            <li>Responsive design techniques (media queries, flexbox, grid)</li>
            <li>Accessibility best practices (ARIA roles, keyboard navigation)</li>
          </ul>

        <strong>Data Source:</strong>
        <ul class="tech-list">
            <li>OpenWeatherMap One Call API (v2.5)(v3.0)</li>
            <li>Open-Meteo API</li>
          </ul>
        <strong>Additional Features:</strong>
          <ul class="tech-list">
            <li>Time zone handling (conversion of UTC timestamps to local time)</li>
            <li>RESTful API design</li>
          </ul>

        <strong>Machine Learning / Data Processing:</strong>
          <ul class="tech-list">
            <li>Feature engineering (handling dew point, rainfall aggregation)</li>
            <li>Model evaluation and validation (cross-validation, performance metrics)</li>
            <li>Prediction serialization and JSON handling</li>
          </ul>

        <strong>Deployment / DevOps:</strong>
          <ul class="tech-list">
            <li>Version control (Git)</li>
            <li>Testing frameworks (pytest for backend, Jest for frontend)</li>
            <li>Continuous integration and deployment (GitHub Actions, Travis CI)</li>
            <li>AWS Cloud Hosting: Route 53, EC2, Elastic Beanstalk, IAM, and associated services used for full web app deployment and management</li>
            <li>Continuous integration and deployment (GitHub Actions, Travis CI)</li>
          </ul>

        <strong>Security:</strong>
          <ul class="tech-list">
            <li>Input validation and sanitization (prevent injection attacks)</li>
            <li>HTTPS / SSL (secure data transmission)</li>
          </ul>


    </div>

    <div class="accordion" id="accordionClimb">
      <div class="accordion-item">
        <h2 class="accordion-header" id="headingClimb">
          <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseClimb" aria-expanded="false" aria-controls="collapseClimb">
            Full Project Details
          </button>
        </h2>
        <div id="collapseClimb" class="accordion-collapse collapse" aria-labelledby="headingClimb" data-bs-parent="#accordionClimb">
          <div class="accordion-body">
            <div class="detail-section">
                <h5>Project Summary</h5>
                <p>This web application predicts location-specific climbing conditions by integrating historical, real-time, and forecasted weather data from the OpenWeatherMap and Open-Meteo APIs. The climbing conditions application uses Random Forest and Decision Tree regression models trained on datasets representing optimal climbing environments to produce a Climbing Conditions Score (CCS). The backend processes weather data in Python, performing time zone adjustments, rainfall aggregation, and predictive modeling before serving results to a responsive frontend.</p>
                <p>The interface, built with JavaScript and Plotly, offers real-time updates, interactive charts, and detailed forecast cards that highlight both CCS values and key meteorological data such as temperature, humidity, rainfall totals, and wind speed. By combining machine learning, robust API processing, and dynamic visualization, the app provides climbers with data-driven insights for evaluating current and upcoming conditions at specific destinations around the United States.</p>
            </div>
            <div class="detail-section">
              <h5>Architecture</h5>
              <h6>Backend (Flask / Python):</h6>
              <ul>
                <li><strong>Weather Data Retrieval</strong>
                  <ul>
                    <li>Uses OpenWeatherMap API to fetch current and future conditions/forecasts</li>
                    <li>Uses open-meteo to fetch historical weather data for personalized user input and model training</li>
                    <li>Forecast cards display daily min/max temperature, humidity, and CCS</li>
                    <li>Converts all dates/times to user’s local time zone using a <code>tz_offset</code> parameter</li>
                  </ul>
                </li>
                <li><strong>Feature Extraction</strong>
                  <ul>
                    <li>Extracts temperature (°C), humidity (%), dew point (°C; retrieved but unused in model)</li>
                    <li>Computes total daily rainfall accumulation for forecast cards</li>
                  </ul>
                </li>
                <li><strong>Machine Learning Prediction</strong>
                  <ul>
                    <li>Loads a pre-trained RandomForestRegressor model (<code>model.pkl</code>) via joblib</li>
                    <li>Inputs: Humidity and Temperature</li>
                    <li>Outputs: Climbing Conditions Score (0–10 scale), higher = better conditions</li>
                    <li>Generates predictions for current conditions and daily forecast periods</li>
                  </ul>
                </li>
                <li><strong>API Endpoints</strong>
                  <ul>
                    <li><code>/all_data</code>: Returns combined current + forecast weather data with CCS in JSON</li>
                    <li>Handles time zone conversion so frontend timestamps are always local</li>
                  </ul>
                </li>
              </ul>

              <h6>Frontend (JavaScript + Plotly):</h6>
              <ul>
                <li><strong>Data Fetching & Rendering</strong>
                  <ul>
                    <li>Detects user’s local time zone offset on load</li>
                    <li>Calls <code>/all_data?tz_offset={offset}</code> to fetch weather + CCS data</li>
                    <li>Updates current conditions widget with CCS score, temperature, humidity, weather description, and local last-update time</li>
                  </ul>
                </li>
                <li><strong>Forecast Cards</strong>
                  <ul>
                    <li>Displays 7-day forecasts with min/max temperature, humidity, predicted CCS, and total rainfall</li>
                    <li>Dates shown in local time zone</li>
                  </ul>
                </li>
                <li><strong>Graphs & Visualization</strong>
                  <ul>
                    <li>Uses Plotly.js for interactive line charts of temperature, humidity, and CCS over time</li>
                    <li>X-axis labels and hover tooltips use local times</li>
                  </ul>
                </li>
                <li><strong>Interactivity</strong>
                  <ul>
                    <li>Swipeable forecast cards</li>
                    <li>Full-screen toggle for charts</li>
                    <li>Smooth data update transitions</li>
                  </ul>
                </li>
              </ul>

              <h5>Machine Learning Model</h5>
              <ul>
                <li><strong>Model Choice:</strong>
                  <ul>
                    <li><strong>Random Forest Regression:</strong>
                      <ul>
                        <li>Robust to non-linear relationships</li>
                        <li>Resistant to overfitting with small to medium datasets</li>
                        <li>Handles mixed seasonal/weather variations without explicit feature engineering</li>
                      </ul>
                    </li>
                    <li><strong>Decision Tree Regression (also supported):</strong>
                      <ul>
                        <li>Simpler, more interpretable model</li>
                        <li>Faster predictions, lower computational cost</li>
                        <li>Easier debugging and understanding of decision paths</li>
                      </ul>
                    </li>
                  </ul>
                </li>
                <li><strong>Features Used:</strong>
                  <ul>
                    <li>Humidity (%) — affects rock surface friction</li>
                    <li>Temperature (°C) — impacts climber comfort and rock conditions</li>
                  </ul>
                </li>
              </ul>

              <h5>Key Technical Features & Challenges Solved</h5>
              <ul>
                <li><strong>Time Zone Handling:</strong> Converts OpenWeatherMap UTC timestamps to user local time on backend using <code>tz_offset</code>, ensuring accurate display in graphs, tooltips, and cards</li>
                <li><strong>Clear Separation of Current vs Forecast Data:</strong> Keeps real-time and forecast predictions distinct for accuracy</li>
                <li><strong>Rainfall Aggregation:</strong> Aggregates hourly precipitation data into total daily rainfall for forecast cards</li>
                <li><strong>Responsive, Mobile-Friendly UI:</strong> Adaptive cards and graphs with swipe navigation</li>
                <li><strong>API Integration Efficiency:</strong> Combines current and daily weather data into a single Flask endpoint to minimize frontend calls</li>
                <li><strong>Personalized Climbing Condition Scores:</strong> Accepts user input where personalized scores can be submitted from cached historical weather conditions, stored in Sqlite database</li>
                <li><strong>Model Improvement:</strong> Uses the user submitted personalized scores to re-train the random forest and decision tree models to make more accurate predictions based on a more inclusive dataset</li>
              </ul>

              <h5>Skills Gained</h5>
              <ul>
                <li>API integration with third-party weather services</li>
                <li>Feature engineering and predictive modeling with scikit-learn</li>
                <li>Full-stack development (Flask backend, JavaScript frontend)</li>
                <li>Data visualization with Plotly</li>
                <li>Time zone management in distributed systems</li>
                <li>Performance optimization for API and rendering</li>
              </ul>
                <div class="detail-section">
                  <h5>Useful Links</h5>
                  <ul class="sub-list">
                    <li><a href="https://github.com/at58474/climbing-conditions-app" target="_blank">GitHub Repo</a></li>
                  </ul>
                </div>
            </div>
          </div>
        </div>
      </div>
    </div>
</div>


  </div>

  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>
